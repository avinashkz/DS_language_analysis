learn_number <- cleaning(learn, "LEARN", rate = FALSE)
help_number <- cleaning(help, "HELP", rate = FALSE)
services_number <- cleaning(services, "SERVICES", rate = FALSE)
# Calculate the confidence interval for the clicks of all the 5 different variants.
conf <- epitools::binom.exact(services_number[3], sum(services_number))
conf <- rbind(conf,epitools::binom.exact(interact_number[3], sum(interact_number)))
conf <- rbind(conf,epitools::binom.exact(learn_number[3], sum(learn_number)))
conf <- rbind(conf,epitools::binom.exact(help_number[3], sum(help_number)))
conf <- rbind(conf,epitools::binom.exact(connect_number[3], sum(connect_number)))
conf$name <- row.names(conf)
#https://stackoverflow.com/questions/14069629/plotting-confidence-intervals?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
#Plot of the confidence interval for all the 5 variants.
conf %>% ggplot() + geom_point(aes(x = name, proportion),size = 3) + geom_errorbar(aes(x = name, ymin = lower, ymax = upper)) + labs(title = "Confidence intervals for CTR", x = "Names", y = "Proportions")
pie.plot <- function(df_raw, variant) {
#Function to plot the pie chart for CTR
ctr <- cleaning(df_raw, variant)
#Reference: https://www.tutorialspoint.com/r/r_pie_charts.htm
pie(ctr, labels = round(ctr,2),
main = paste("Click-through rates - Homepage,", variant),
col = rainbow(length(ctr)))
legend("topright",names(ctr), cex = 0.8, fill = rainbow(length(ctr)))
}
#Plotting the pie charts
pie.plot(interact, "INTERACT")
pie.plot(connect, "CONNECT")
pie.plot(learn, "LEARN")
pie.plot(help, "HELP")
pie.plot(services, "SERVICES")
click_df <- cbind(interact_ctr[3]/100,sum(interact_number))
click_df <- rbind(click_df,cbind(services_ctr[3]/100,sum(services_number)))
click_df <- rbind(click_df,cbind(connect_ctr[3]/100,sum(connect_number)))
click_df <- rbind(click_df,cbind(learn_ctr[3]/100,sum(learn_number)))
click_df <- rbind(click_df,cbind(help_ctr[3]/100,sum(help_number)))
click_df <- click_df %>% as.data.frame()
names(click_df) <- c("clicks", "total")
click_df <- cbind(click_df,variation = row.names(click_df))
model.data <- data_frame()
set.seed(99)
for (i in 1:length(click_df[,1])){
model.data <- rbind(model.data,cbind(rbinom(n = click_df[i,]$total ,prob = click_df[i,]$clicks, size = 1), as.character(click_df[i,]$variation)))
}
colnames(model.data) <- c("click", "name")
model <- glm(click ~ name, family = binomial(link = 'logit'), data = model.data)
summary(model)
p.vals <- summary(model)$coef[,4]
p.adjust(p.vals ,method = "BH") < 0.05
pwr::pwr.anova.test(k = 5, power = 0.8, f = 0.4)
#power.prop.test(power = 0.8, p1 = 0.02847709 , p2 = 0.02821317)
#power.prop.test(power = 0.8, p1 = 0.02847709 , p2 = 0.03906742)
library(rjags)
click_count <- data_frame()
click_count <- cbind(interact_number[3],sum(interact_number))
click_count <- rbind(click_count,cbind(services_number[3],sum(services_number)))
click_count <- rbind(click_count,cbind(connect_number[3],sum(connect_number)))
click_count <- rbind(click_count,cbind(learn_number[3],sum(learn_number)))
click_count <- rbind(click_count,cbind(help_number[3],sum(help_number)))
click_count <- click_count %>% as.data.frame()
names(click_count) <- c("clicks", "total")
click_count <- cbind(click_count,variation = row.names(click_count))
#Computing the posterior distribution using jags
my_model <- "
model{
for (i in 1:N){
#Prior distribution
theta[i] ~ dbeta(1, 1)
#Statistic model
ll[i] ~ dbinom(theta[i], n[i])
}
}
"
mod <- jags.model(textConnection(my_model), data=list(ll = c(click_count$clicks), n = c(click_count$total), N = 5))
jags_coin <- coda.samples(mod, variable.names=c("theta"), n.iter=10000)
plot(jags_coin)
beta.est <- summary(jags_coin)$statistics[,"Mean"][1:5]
beta.int <- t(apply(as.matrix(jags_coin), 2, quantile, prob=c(.025,.975)))
plot(1:5, beta.est, pch=20, ylim=c(0,0.07),ylab=expression(beta),
xlab="", main="95% ET credible intervals")
for (i in 1:5) {
points(rep(i,2), beta.int[i,], type="l",
col=c("black","red")[1+as.numeric((beta.int[i,1]*beta.int[i,2])>0)])
}
#The order of the confidence intervals are as given below
row.names(click_count)
model.data
model <- glm(click ~ name, family = binomial(link = 'logit'), data = model.data)
data
data <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))%>% select(preference, binary, task, "background", "active", first)
data
mod <- glm(binary ~ task, data = data)
summary(mod)
summary(model)
mod <- glm(binary ~ task + first, data = data)
summary(mod)
mod <- glm(binary ~ task + first + background, data = data)
summary(mod)
responses
data <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))%>% select(preference, binary, task, "background", "active", first, attitude)
mod <- glm(binary ~ task + first + background + attitude, data = data)
summary(mod)
model <- glm(binary ~ task + first + background + attitude, data = data)
summary(mod)
p.vals <- summary(model)$coef[,4]
p.adjust(p.vals ,method = "BH") < 0.05
model <- glm(binary ~ task + first + background + attitude, data = data)
summary(mod)
p.vals <- summary(model)$coef[,4]
p.adjust(p.vals ,method = "BH") < 0.05
p.adjust(p.vals ,method = "BH") < 0.05
model <- glm(binary ~ task + first, data = data)
summary(mod)
p.vals <- summary(model)$coef[,4]
p.adjust(p.vals ,method = "BH") < 0.05
model <- glm(binary ~ task + first, data = data)
summary(mod)
names(responses)
data <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))%>% select(preference, binary, task, "background", "active", first, attitude)
data
mod <- glm(binary ~ task, data = data)
summary(mod)
model <- glm(binary ~ task + first, data = data)
summary(mod)
summary(model)
model <- glm(binary ~ task + attitude, data = data)
summary(model)
model <- glm(binary ~ task + attitude + first + background, data = data)
summary(model)
knitr::opts_chunk$set(echo = FALSE)
source('get_plots.R')
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(cowplot))
suppressPackageStartupMessages(library(ggalluvial))
responses <- read.csv(file = "../docs/survey_results_clean.csv")
plot_title <- c("Academic Background", "Years of Coding Experience", "User Who Love/Enjoy Coding", "First Programming Language",
"R or Python", "Preferred Data Science Task", "Number of Languages Actively Being Used")
plot_colors <- c("red", "blue4")
#R or Python !!
pie_plot(responses[,c(5,5)], 5, plot_title, colnames(responses[,5]))
#Preferred Data Science Task
pie_plot(responses[,c(6,5)], 6, plot_title, colnames(responses[,6]))
responses %>%
ggplot() +
geom_bar(aes(x = preference,fill = preference), alpha = 0.7) +
facet_wrap(~task) + theme_bw() +
scale_fill_manual(name = "Choice of languge", values = plot_colors) +
ggtitle("Facet Plot for Preferred Data Science Task") + xlab("Choice of language") + ylab("Count") +
theme(plot.title = element_text(hjust = 0.5,size=16))
#Academic Background
#proportion_plot(responses[,c(1,5)], 1, plot_title, colnames(responses[,1]))
bar_plot(responses[,c(1,5)], 1, plot_title, colnames(responses[,1]))
grouped_df <- responses %>% group_by(background, preference, task) %>% summarise(count = n())
ggplot(grouped_df,
aes(weight = count,
axis1 = task, axis2 = background, axis3 = preference)) +
geom_alluvium(aes(fill = task),
width = 0) +
guides(fill = FALSE) +
geom_stratum(reverse = TRUE, alpha = 0.6) +
geom_text(stat = "stratum", label.strata = TRUE, reverse = TRUE, size = 3) +
scale_x_continuous(breaks = 1:3, labels = c("Preferred Task", "Background", "Language")) +
ggtitle("Alluvial Plot for Language Preference") + theme(axis.text.y = element_blank()) + theme_void()+
theme(plot.title = element_text(hjust = 0.5,size=16))
#Years of Coding Experience
pie_plot(responses[,c(2,5)], 2, plot_title, colnames(responses[,2]))
responses %>%
ggplot() +
geom_bar(aes(x = preference,fill = preference), alpha = 0.7) +
facet_wrap(~experience) + theme_bw() +
scale_fill_manual(name = "Choice of languge", values = plot_colors) +
ggtitle("Facet plot for Years of Coding Experience") + xlab("Years of Coding Experience") + ylab("Count") +
theme(plot.title = element_text(hjust = 0.5, size=16))
#Attitude Towards
pie_plot(responses[,c(3,5)], 3, plot_title, colnames(responses[,3]))
responses %>%
ggplot() +
geom_bar(aes(x = preference, fill = preference), alpha = 0.7) +
facet_wrap(~attitude) + theme_bw() +
scale_fill_manual(name = "Choice of languge", values = plot_colors) +
ggtitle("Facet Plot for User Who Love/Enjoy Coding") + xlab("Choice of language") + ylab("Count") +
theme(plot.title = element_text(hjust = 0.5, size=16))
#First Programming Language
#proportion_plot(responses[,c(4,5)], 4, plot_title, colnames(responses[,4]))
bar_plot(responses[,c(4,5)], 4, plot_title, colnames(responses[,4]), flip = FALSE)
#Number of Languages Actively Being Used
#proportion_plot(responses[,c(6,5)], 6, plot_title, colnames(responses[,7]))
bar_plot(responses[,c(7,5)], 7, plot_title, colnames(responses[,7]), flip = FALSE)
responses %>% colnames()
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = responses)
data_all <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data_all)
summary(mod)
mod <- glm(binary ~ task + first + background + experience + attitude + active, data = data_all)
summary(mod)
mod <- glm(binary ~ task + first + background + experience + attitude, data = data_all)
summary(mod)
mod <- glm(binary ~ task + first + background, data = data_all)
summary(mod)
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data_all)
summary(mod)
model <- glm(binary ~ task + first, data = data)
summary(model)
relevel(data$first,ref="Matlab")
data_relevel <- data
data_relevel$first <-relevel(data$first,ref="Matlab")
model <- glm(binary ~ task + first, data = data)
summary(model)
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
data_relevel <- data
ata_relevel$task
data_relevel$task
data_relevel$task <-relevel(data$task,ref="Machine Learning")
model <- glm(binary ~ task + first, data = data)
summary(model)
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
data_relevel <- data
data_relevel <- data
data_relevel$task <-relevel(data$task,ref="Machine Learning")
data_relevel$first <-relevel(data$first,ref="R")
model <- glm(binary ~ task + first, data = data)
summary(model)
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
responses %>% colnames()
#All the data with
data <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))
responses %>% colnames()
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data)
summary(mod)
mod <- glm(binary ~ task + first + background, data = data_all)
summary(mod)
model <- glm(binary ~ task + attitude + first + background, data = data)
summary(model)
#All the data with
data <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))
responses %>% colnames()
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data)
summary(mod)
#Model with only first language or background
mod <- glm(binary ~ task + first + background, data = data_all)
summary(mod)
#Releveling the reference task from Data Viz -> Machine Learning
#Releveling the reference first language from C -> R
data_relevel <- data
data_relevel$task <-relevel(data$task,ref="Machine Learning")
data_relevel$first <-relevel(data$first,ref="R")
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
#Binary encoding the response variable. Python -> 1; R -> 0
data <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))
#Fitting a GLM without any confounding variables.
mod <- glm(binary ~ task, data = data)
summary(mod)
model <- glm(binary ~ task + first, data = data)
summary(model)
#Adjusting for p-values.(Not required anymore after chat with Tiffany)
#p.vals <- summary(model)$coef[,4]
#p.adjust(p.vals ,method = "BH") < 0.05
knitr::opts_chunk$set(echo = TRUE)
#Loading the required packages
suppressPackageStartupMessages(library(tidyverse))
#Reading in processed data.
responses <- read.csv(file = "docs/survey_results_clean.csv")
getwd()
#Reading in processed data.
responses <- read.csv(file = "docs/survey_results_clean.csv")
#Reading in processed data.
responses <- read.csv(file = "../docs/survey_results_clean.csv")
knitr::opts_chunk$set(echo = TRUE)
#Loading the required packages
suppressPackageStartupMessages(library(tidyverse))
#Reading in processed data.
responses <- read.csv(file = "../docs/survey_results_clean.csv")
#Binary encoding the response variable. Python -> 1; R -> 0
data <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))
#Fitting a GLM without any confounding variables.
mod <- glm(binary ~ task, data = data)
summary(mod)
#Fitting GLM with all the confounding variables.
responses %>% colnames()
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data)
summary(mod)
#Model with only first language or background
mod <- glm(binary ~ task + first + background, data = data_all)
#Fitting GLM with all the confounding variables.
responses %>% colnames()
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data)
summary(mod)
#Model with only first language or background
mod <- glm(binary ~ task + first + background, data = data)
summary(mod)
model <- glm(binary ~ task + first, data = data)
summary(model)
#Adjusting for p-values.(Not required anymore after chat with Tiffany)
#p.vals <- summary(model)$coef[,4]
#p.adjust(p.vals ,method = "BH") < 0.05
#Releveling the reference task from Data Viz -> Machine Learning
#Releveling the reference first language from C -> R
data_relevel <- data
data_relevel$task <-relevel(data$task,ref="Machine Learning")
data_relevel$first <-relevel(data$first,ref="R")
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
#Releveling 1
#Releveling the reference task from Data Viz -> Machine Learning
#Not Releveling the reference first language from C -> R
data_relevel <- data
data_relevel$task <-relevel(data$task,ref="Machine Learning")
#data_relevel$first <-relevel(data$first,ref="R")
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
model1 <- glm(binary ~ task, data = data_relevel)
summary(mode1l)
summary(model1)
model2 <- glm(binary ~ task, data = data)
summary(model2)
#Releveling the reference task from Data Viz -> Machine Learning
data_relevel <- data
data_relevel$task <-relevel(data$task,ref="Machine Learning")
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
#Releveling the reference task from Data Viz -> Machine Learning
data_relevel <- data
data_relevel$task <-relevel(data$task,ref="Data wrangling")
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
#Releveling the reference task from Data Viz -> Machine Learning
data_relevel <- data
data_relevel$task <-relevel(data$task,ref="Machine Learning")
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
#Fitting GLM with all the confounding variables.
responses %>% colnames()
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data)
summary(mod)
#Binary encoding the response variable. Python -> 1; R -> 0
data <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))
#Releveling the reference task from Data Viz -> Machine Learning
data_relevel <- data
data_relevel$task <-relevel(data$task,ref="Machine Learning")
#Fitting a GLM without any confounding variables.
mod <- glm(binary ~ task, data = data)
summary(mod)
mod <- glm(binary ~ task, data = data_relevel)
summary(mod)
#Binary encoding the response variable. Python -> 1; R -> 0
data <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))
#Releveling the reference task from Data Viz -> Machine Learning
data_relevel <- data
data_relevel$task <-relevel(data$task,ref="Machine Learning")
#Fitting a GLM without any confounding variables.
mod <- glm(binary ~ task, data = data)
summary(mod)
mod <- glm(binary ~ task, data = data_relevel)
summary(mod)
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data_relevel)
summary(mod)
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data_relevel)
summary(mod)
#Removing Attitude as Confounder as change
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data)
summary(mod)
#Removing Attitude as Confounder as change
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data_relevel)
summary(mod)
#Removing Attitude as Confounder as change
mod <- glm(binary ~ task + background + experience + first + active, data = data_relevel)
summary(mod)
#Removing First language as Confounder as change
mod <- glm(binary ~ task + background + experience + active, data = data)
summary(mod)
#Removing First language as Confounder as change
mod <- glm(binary ~ task + background + experience + active, data = data_relevel)
summary(mod)
#Fitting GLM with all the confounding variables.
responses %>% colnames()
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data)
summary(mod)
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data_relevel)
summary(mod)
#Removing Attitude as Confounder as change
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data)
summary(mod)
#Removing Experience as Confounder
mod <- glm(binary ~ task + background + first + active, data = data)
summary(mod)
#Removing Experience as Confounder
mod <- glm(binary ~ task + background + first + active, data = data_relevel)
summary(mod)
#Removing Experience as Confounder
mod <- glm(binary ~ task + background + first + active, data = data)
summary(mod)
#Removing Experience as Confounder
mod <- glm(binary ~ task + background + first + active, data = data_relevel)
summary(mod)
#Removing active as Confounder
mod <- glm(binary ~ task + background + first, data = data)
summary(mod)
#Removing active as Confounder
mod <- glm(binary ~ task + background + first, data = data_relevel)
summary(mod)
#Removing active as Confounder
mod <- glm(binary ~ task + first + background, data = data_relevel)
summary(mod)
#Removing active as Confounder
mod <- glm(binary ~ task + background + first, data = data_relevel)
summary(mod)
#Removing first as Confounder
mod <- glm(binary ~ task + background, data = data)
summary(mod)
#Removing first as Confounder
mod <- glm(binary ~ task + background, data = data_relevel)
summary(mod)
#Removing first as Confounder
mod <- glm(binary ~ task + background, data = data_relevel)
summary(mod)
#Final Model
#Model with first language and background
mod <- glm(binary ~ task + background + first, data = data)
summary(mod)
model <- glm(binary ~ task + background + first, data = data_relevel)
summary(model)
#Adjusting for p-values.(Not required anymore after chat with Tiffany)
#p.vals <- summary(model)$coef[,4]
#p.adjust(p.vals ,method = "BH") < 0.05
#Removing background as Confounder
mod <- glm(binary ~ task + first, data = data)
summary(mod)
#Removing background as Confounder
mod <- glm(binary ~ task + first, data = data_relevel)
summary(mod)
#Model with first language and background
mod <- glm(binary ~ task + first, data = data)
summary(mod)
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
#Adjusting for p-values.(Not required anymore after chat with Tiffany)
#p.vals <- summary(model)$coef[,4]
#p.adjust(p.vals ,method = "BH") < 0.05
#Model with first language and background
mod <- glm(binary ~ task + experince, data = data)
#Model with first language and background
mod <- glm(binary ~ task + expereince, data = data)
#Model with first language and background
mod <- glm(binary ~ task + experaince, data = data)
data
#Model with first language and background
mod <- glm(binary ~ task + experience, data = data)
summary(mod)
data
#Model with first language and background
mod <- glm(binary ~ task + attitude, data = data)
summary(mod)
knitr::opts_chunk$set(echo = TRUE)
#Loading the required packages
suppressPackageStartupMessages(library(tidyverse))
#Reading in processed data.
responses <- read.csv(file = "../docs/survey_results_clean.csv")
#Binary encoding the response variable. Python -> 1; R -> 0
data <- responses %>% mutate(binary = if_else(preference == "Python", 1, 0))
#Releveling the reference task from Data Viz -> Machine Learning
data_relevel <- data
data_relevel$task <-relevel(data$task,ref="Machine Learning")
#Fitting a GLM without any confounding variables.
mod <- glm(binary ~ task, data = data)
summary(mod)
mod <- glm(binary ~ task, data = data_relevel)
summary(mod)
#Fitting GLM with all the confounding variables.
responses %>% colnames()
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data)
summary(mod)
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data_relevel)
summary(mod)
#Removing Attitude as Confounder as change
mod <- glm(binary ~ task + background + experience + attitude + first + active, data = data)
summary(mod)
#Removing Attitude as Confounder as change
mod <- glm(binary ~ task + background + experience + first + active, data = data_relevel)
summary(mod)
#Removing Experience as Confounder
mod <- glm(binary ~ task + background + first + active, data = data)
summary(mod)
#Removing Experience as Confounder
mod <- glm(binary ~ task + background + first + active, data = data_relevel)
summary(mod)
#Removing active as Confounder
mod <- glm(binary ~ task + background + first, data = data)
summary(mod)
#Removing active as Confounder
mod <- glm(binary ~ task + background + first, data = data_relevel)
summary(mod)
#Removing first as Confounder
mod <- glm(binary ~ task + background, data = data)
summary(mod)
#Removing first as Confounder
mod <- glm(binary ~ task + background, data = data_relevel)
summary(mod)
#Removing background as Confounder
mod <- glm(binary ~ task + first, data = data)
summary(mod)
#Removing background as Confounder
mod <- glm(binary ~ task + first, data = data_relevel)
summary(mod)
#Model with first language and background
mod <- glm(binary ~ task + first, data = data)
summary(mod)
model <- glm(binary ~ task + first, data = data_relevel)
summary(model)
#Adjusting for p-values.(Not required anymore after chat with Tiffany)
#p.vals <- summary(model)$coef[,4]
#p.adjust(p.vals ,method = "BH") < 0.05
